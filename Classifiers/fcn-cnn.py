# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14FvkVPbN4zXOVvKg_e0sl5LPXu6Gvn_G
"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Activation,Flatten,Conv2D,Reshape,Dropout,MaxPooling2D,BatchNormalization,GlobalAveragePooling2D
from sklearn.preprocessing import LabelBinarizer as LB
from sklearn.preprocessing import normalize
from keras.callbacks import EarlyStopping
import tensorflow as tf
import numpy as np
import pickle
! pip install wget
import wget
from sklearn import metrics
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import gc
from sklearn.model_selection import train_test_split

# ! --> for running shell command .. run once to download data
! wget http://opendata.deepsig.io/datasets/2016.10/RML2016.10b.tar.bz2
! tar jxf RML2016.10b.tar.bz2

# Xd is a dictionnary of data:
# its format is (mod,snr) : ndarray of shape(6000,2,128)
# in other word its format is (b'8PSK',2) : ndarray (6000 : each 2 row and 128 col)
Xd = pickle.load(open("RML2016.10b.dat",'rb'), encoding='bytes')

# snrs values are : [-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]
# mods values are : [b'8PSK', b'AM-DSB', b'BPSK', b'CPFSK', b'GFSK', b'PAM4', b'QAM16', b'QAM64', b'QPSK', b'WBFM']
snrs, mods = map(lambda j: sorted(list(set(map(lambda x: x[j], Xd.keys())))), [1, 0])
print(snrs)
print(mods)


# get data & its label
# data will be the ndarray of a certain combination of (snr,mod)
# we have 6000*200 sample, each sample have an 2*128 ndarray value
# so basically up to this point X = (1200000, 2, 128)
X = []
lbl = []
for mod in mods:
    for snr in snrs:
        X.append(Xd[(mod, snr)])
        for i in range(Xd[(mod, snr)].shape[0]):
          lbl.append((mod, snr))
X = np.vstack(X)

################################################################################
# here we make a dictionnary of features for easy access later
features = {}
# Raw Time Feature
features['raw'] = X[:, 0], X[:, 1]
# First derivative in time
features['derivative'] = normalize(np.gradient(X[:, 0], axis=1)), normalize(np.gradient(X[:, 1], axis=1))
# Integral in time
features['integral'] = normalize(np.cumsum(X[:, 0], axis=1)), normalize(np.cumsum(X[:, 1], axis=1))

# All Together Feature Space 
# hint : desired will be like ("apple", "banana", "cherry", "orange", "kiwi") 
def extract_features(*arguments):
    desired = ()
    for arg in arguments:
        desired = desired + features[arg]
    return np.stack(desired, axis=1)

# Init general case
# feature choice -> raw
data = extract_features('raw')
labels = np.array(lbl)


 
np.random.seed(10)
n_examples = labels.shape[0]
r = np.random.choice(range(n_examples), n_examples, replace=False)
train_examples = r[0:int(n_examples*0.7)]
test_examples = r[int(n_examples*0.7)::]
X_train = data[train_examples]
X_test = data[test_examples]
in_shape = data[0].shape
out_shape = tuple([1]) + in_shape
y_train = LB().fit_transform(labels[train_examples][:, 0])
y_test = LB().fit_transform(labels[test_examples][:, 0])
snr_train = labels[train_examples][:, 1].astype(int)
snr_test = labels[test_examples][:, 1].astype(int)

class AccuracyHistory(tf.keras.callbacks.Callback):
    def on_train_begin(self, logs={}):
        self.acc = []

    def on_epoch_end(self, batch, logs={}):
        self.acc.append(logs.get('acc'))

history = AccuracyHistory()
print(history)

#FC MODEL

hidden1_size = 512
hidden2_size = 256
hidden3_size = 10

  # Build model
modelfc = tf.keras.Sequential()
modelfc.add(Dense(hidden1_size, input_shape = in_shape, kernel_initializer='he_normal'))
modelfc.add(Activation('relu'))
modelfc.add(Dense(hidden2_size, kernel_initializer='he_normal'))
modelfc.add(Activation('relu'))
modelfc.add(Flatten())
modelfc.add(Dense(hidden3_size))
modelfc.add(Activation('softmax'))
  # Compile model
modelfc.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
modelfc.summary()

modelfc.fit(X_train, y_train,batch_size=2048,epochs=100,verbose=1,validation_split=0.05,
          callbacks=[history])

scorefc = modelfc.evaluate(X_test, y_test, verbose=0)
print('Test loss:', scorefc[0])
print('Test accuracy:', scorefc[1])





acc_fcn=[]

X = []
x_extra = []
lbl = []
lbl_extra = []
for snr in snrs:
  for mod in mods :
    x_extra.append(Xd[(mod,snr)])
    for i in range(Xd[(mod,snr)].shape[0]):
      lbl_extra.append(mod)
  X.append(np.vstack(x_extra[:]))
  x_extra.clear()
  lbl.append(lbl_extra[:])
  lbl_extra.clear()

X= np.array(X)
lbl = np.vstack(lbl)
del(x_extra)
del(lbl_extra)
gc.collect()

for i in range (0,20):
  training_data, test_data, training_labels, test_labels = train_test_split(X[i],lbl[i],test_size = 0.3,random_state = 42)
  y_pred = modelfc.predict(test_data)
  thresh = 0.5
  y_prednew=[]
  #y_pred = np.array([[mod[y_pred[i].index(max(y_pred[i]))] for i in j] for j in y_pred])
  for k in range (0,len(y_pred)):
    y_prednew.append(mods[np.argmax(y_pred[k])])
  print(y_prednew)
  ac = metrics.accuracy_score(test_labels, y_prednew)
  acc_fcn.append(ac)
  # if snrs[i]==0:
  cm = metrics.confusion_matrix(test_labels, y_prednew)
  cm_df = pd.DataFrame(cm, index = mods, columns= mods)
  plt.figure(figsize=(5.5,4))
  sns.heatmap(cm_df)
  plt.title(f'fcn_cm_snr={snrs[i]}\nAccuracy:{ac}')
  plt.show()
  gc.collect()

plt.figure()
fig1, = plt.plot(snrs,acc_fcn,'bo')
plt.legend((fig1), ('FCN'))
plt.show()

print (f'average of FCN={sum(acc_fcn)/len(acc_fcn)}')

#CNN MODEL\n",
conv1_kernel_shape=(3,1)
conv1_number_of_filters=64
conv2_kernel_shape=(3,2)
conv2_number_of_filters=16
dense1_size = 128
dense2_size = 10
dropout = 0.4

model = tf.keras.Sequential()

model.add(Reshape((128,in_shape[0],1), input_shape=in_shape))
model.add(Conv2D(conv1_number_of_filters, conv1_kernel_shape, strides=1,
padding='same', data_format='channels_last', activation='relu', kernel_initializer='he_normal'))
model.add(BatchNormalization())
model.add(MaxPooling2D())
model.add(Conv2D(conv2_number_of_filters, conv2_kernel_shape, strides=1,
                   padding='same', data_format='channels_last', activation='relu', kernel_initializer='he_normal'))
model.add(Flatten())
model.add(Dropout(rate=1-dropout))
model.add(Dense(dense1_size, activation='relu', kernel_initializer='he_normal'))
model.add(Dense(dense2_size, activation='softmax'))

#Compile model\n",
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()

model.fit(X_train, y_train,batch_size=2048,epochs=20,verbose=1,validation_split=0.05,
        callbacks=[history])

acc_cnn=[]
for i in range (0,20):
  training_data, test_data, training_labels, test_labels = train_test_split(X[i],lbl[i],test_size = 0.3,random_state = 42)
  y_pred = model.predict(test_data)
  y_prednew=[]
  for k in range (0,len(y_pred)):
    y_prednew.append(mods[np.argmax(y_pred[k])])
  print(y_prednew)
  ac = metrics.accuracy_score(test_labels, y_prednew)
  acc_cnn.append(ac)
  # if snrs[i]==0:
  cm = metrics.confusion_matrix(test_labels, y_prednew)
  cm_df = pd.DataFrame(cm, index = mods, columns= mods)
  plt.figure(figsize=(5.5,4))
  sns.heatmap(cm_df)
  plt.title(f'ccn_cm_snr={snrs[i]}\nAccuracy:{ac}')
  plt.show()
  gc.collect()

plt.figure()
fig1, = plt.plot(snrs,acc_ccn)
plt.legend((fig1), ('CCN'))
plt.show()